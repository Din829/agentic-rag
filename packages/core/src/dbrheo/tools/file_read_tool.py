"""
æ–‡ä»¶è¯»å–å·¥å…· - è®©Agentèƒ½å¤Ÿè¯»å–SQLè„šæœ¬ã€é…ç½®æ–‡ä»¶ç­‰
å€Ÿé‰´Gemini CLIçš„åˆ†é¡µè¯»å–ã€å¤šåª’ä½“æ”¯æŒç­‰è®¾è®¡
"""

import os
import json
import yaml
import base64
import mimetypes
from pathlib import Path
from typing import Dict, Any, Optional, List
from ..types.tool_types import ToolResult
from ..types.core_types import AbortSignal
from ..types.file_types import FileFormat, FileAnalysisResult
from .base import Tool
from ..config.base import AgentConfig
from ..utils.debug_logger import DebugLogger, log_info


class FileReadTool(Tool):
    """
    å¢å¼ºçš„æ–‡ä»¶è¯»å–å·¥å…·ï¼Œæ”¯æŒï¼š
    - SQLè„šæœ¬ã€é…ç½®æ–‡ä»¶ã€CSVæ•°æ®ç­‰æ–‡æœ¬æ–‡ä»¶
    - åˆ†é¡µè¯»å–å¤§æ–‡ä»¶ï¼ˆå€Ÿé‰´Gemini CLIï¼‰
    - å›¾ç‰‡å’ŒäºŒè¿›åˆ¶æ–‡ä»¶çš„æ™ºèƒ½å¤„ç†
    - æ–‡ä»¶å†…å®¹åˆ†æå’Œç»“æ„æå–
    """
    
    # æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶æ‰©å±•å
    TEXT_EXTENSIONS = {
        '.sql', '.json', '.yaml', '.yml', '.md', '.txt', 
        '.csv', '.tsv', '.ini', '.conf', '.config', '.env',
        '.xml', '.html', '.log', '.sh', '.py', '.js'
    }
    
    # å›¾ç‰‡æ ¼å¼ï¼ˆå€Ÿé‰´Gemini CLIï¼‰
    IMAGE_EXTENSIONS = {
        '.png', '.jpg', '.jpeg', '.gif', '.webp', '.svg', '.bmp', '.ico'
    }
    
    # æœ€å¤§æ–‡ä»¶å¤§å° (50MB for text, 10MB for images)
    MAX_TEXT_FILE_SIZE = 50 * 1024 * 1024
    MAX_IMAGE_FILE_SIZE = 10 * 1024 * 1024
    
    # é»˜è®¤è¡Œæ•°é™åˆ¶ï¼ˆå€Ÿé‰´Gemini CLIçš„2000è¡Œï¼‰
    DEFAULT_LINE_LIMIT = 2000
    MAX_LINE_LENGTH = 2000
    
    def __init__(self, config: AgentConfig, i18n=None):
        # å…ˆä¿å­˜i18nå®ä¾‹ï¼Œä»¥ä¾¿åœ¨åˆå§‹åŒ–æ—¶ä½¿ç”¨
        self._i18n = i18n
        
        super().__init__(
            name="read_file",
            display_name=self._('file_read_tool_name', default="æ–‡ä»¶è¯»å–") if i18n else "æ–‡ä»¶è¯»å–",
            description="Reads files with intelligent format detection and pagination support. When file not found: automatically lists directory contents, searches for similar filenames, and attempts path corrections. IMPORTANT: If user specifies a line limit, only read that many lines and wait for further instructions, do NOT automatically continue reading.",
            parameter_schema={
                "type": "object",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Absolute path to the file to read"
                    },
                    "encoding": {
                        "type": "string",
                        "description": "File encoding (auto-detected if not specified). Common: utf-8, cp932 (Japanese Windows), gbk (Chinese)",
                        "default": "auto"
                    },
                    "offset": {
                        "type": "integer",
                        "description": "Start reading from line N (0-based, for pagination)",
                        "minimum": 0
                    },
                    "limit": {
                        "type": "integer",
                        "description": "Maximum number of lines to read (default: 2000)",
                        "minimum": 1,
                        "maximum": 10000
                    },
                    "analyze": {
                        "type": "boolean",
                        "description": "Analyze file structure and content (for CSV/JSON)",
                        "default": False
                    }
                },
                "required": ["path"]
            },
            is_output_markdown=True,
            can_update_output=True,
            should_summarize_display=True,
            i18n=i18n  # ä¼ é€’i18nç»™åŸºç±»
        )
        self.config = config
        # åŠ¨æ€æ£€æµ‹ç³»ç»Ÿå¹¶è®¾ç½®çµæ´»çš„è®¿é—®æƒé™
        default_paths = self._get_system_paths(config)
        self.allowed_paths = config.get("file_allowed_paths", default_paths)
    
    def validate_tool_params(self, params: Dict[str, Any]) -> Optional[str]:
        """éªŒè¯å‚æ•°"""
        path = params.get("path", "")
        if not path:
            return self._('file_read_path_empty', default="File path cannot be empty")
        
        # å¿…é¡»æ˜¯ç»å¯¹è·¯å¾„ï¼ˆå€Ÿé‰´Gemini CLIï¼‰
        if not os.path.isabs(path):
            return self._('file_read_path_not_absolute', default="Path must be absolute")
        
        return None
    
    def _get_system_paths(self, config) -> list:
        """åŠ¨æ€æ£€æµ‹ç³»ç»Ÿå¹¶è¿”å›åˆé€‚çš„è®¿é—®è·¯å¾„ - çœŸæ­£çš„çµæ´»æ€§"""
        paths = []
        
        # æ™ºèƒ½æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•ï¼ˆå¾€ä¸Šæ‰¾åˆ°åŒ…å«packagesçš„ç›®å½•ï¼‰
        working_dir = Path(config.get_working_dir())
        current_dir = working_dir
        
        # å‘ä¸ŠæŸ¥æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
        while current_dir.parent != current_dir:  # æ²¡åˆ°æ ¹ç›®å½•
            if (current_dir / 'packages').exists() or (current_dir / 'pyproject.toml').exists():
                paths.append(str(current_dir))
                break
            current_dir = current_dir.parent
        
        # å¦‚æœæ²¡æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œè‡³å°‘åŒ…å«å·¥ä½œç›®å½•
        paths.append(config.get_working_dir())
        
        # ç”¨æˆ·ä¸»ç›®å½• - è·¨å¹³å°é€šç”¨
        home_dir = os.path.expanduser("~")
        if home_dir and os.path.exists(home_dir):
            paths.append(home_dir)
        
        # æ ¹æ®ç³»ç»Ÿå¹³å°åŠ¨æ€æ·»åŠ æ ¹è·¯å¾„
        import platform
        system = platform.system().lower()
        
        if system == "windows":
            # Windows: åŠ¨æ€æ£€æµ‹æ‰€æœ‰å¯ç”¨é©±åŠ¨å™¨
            import string
            for drive in string.ascii_uppercase:
                drive_path = f"{drive}:\\"
                if os.path.exists(drive_path):
                    paths.append(drive_path)
        
        elif system == "darwin":  # macOS
            paths.extend([
                "/",              # æ ¹ç›®å½•
                "/Users",         # ç”¨æˆ·ç›®å½•
                "/Applications",  # åº”ç”¨ç¨‹åº
                "/Volumes",       # æŒ‚è½½ç‚¹
            ])
        
        elif system == "linux":
            paths.extend([
                "/",              # æ ¹ç›®å½•
                "/home",          # ç”¨æˆ·ç›®å½•
                "/mnt",           # æŒ‚è½½ç‚¹ (WSLç­‰)
                "/media",         # åª’ä½“æŒ‚è½½
                "/opt",           # å¯é€‰è½¯ä»¶
                "/tmp",           # ä¸´æ—¶ç›®å½•
            ])
        
        else:
            # æœªçŸ¥ç³»ç»Ÿï¼Œä½¿ç”¨é€šç”¨è·¯å¾„
            if os.path.exists("/"):
                paths.append("/")
            # å°è¯•æ£€æµ‹å¸¸è§æŒ‚è½½ç‚¹
            for mount_point in ["/mnt", "/media", "/Volumes"]:
                if os.path.exists(mount_point):
                    paths.append(mount_point)
        
        # è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„è·¯å¾„ï¼Œä¿ç•™çœŸå®å¯è®¿é—®çš„
        return [p for p in paths if os.path.exists(p)]
    
    def get_description(self, params: Dict[str, Any]) -> str:
        """è·å–æ“ä½œæè¿°"""
        path = Path(params.get("path", ""))
        offset = params.get("offset", 0)
        limit = params.get("limit", self.DEFAULT_LINE_LIMIT)
        
        desc = self._('file_read_description', default="Read file: {filename}", filename=path.name)
        if offset > 0:
            desc += self._('file_read_offset_suffix', default=" (from line {line})", line=offset + 1)
        if limit < self.DEFAULT_LINE_LIMIT:
            desc += self._('file_read_limit_suffix', default=" (limit {limit} lines)", limit=limit)
        
        return desc
    
    async def should_confirm_execute(self, params: Dict[str, Any], signal: AbortSignal) -> Optional[Any]:
        """è¯»å–æ–‡ä»¶é€šå¸¸ä¸éœ€è¦ç¡®è®¤"""
        return False
        
    async def execute(
        self,
        params: Dict[str, Any],
        signal: AbortSignal,
        update_output: Optional[Any] = None
    ) -> ToolResult:
        """æ‰§è¡Œæ–‡ä»¶è¯»å–"""
        file_path = params.get("path", "")
        encoding_param = params.get("encoding", "auto")
        # ç¡®ä¿ offset å’Œ limit æ˜¯æ•´æ•°ï¼ˆGemini API å¯èƒ½ä¼ é€’å­—ç¬¦ä¸²ï¼‰
        offset = int(params.get("offset", 0)) if params.get("offset") is not None else 0
        limit = int(params.get("limit", self.DEFAULT_LINE_LIMIT)) if params.get("limit") is not None else self.DEFAULT_LINE_LIMIT
        analyze = bool(params.get("analyze", False))
        
        # å¤„ç†ç¼–ç  - æ”¯æŒè‡ªåŠ¨æ£€æµ‹
        if encoding_param == "auto":
            encoding = await self._detect_encoding(file_path)
        else:
            encoding = encoding_param
        
        try:
            # è§„èŒƒåŒ–è·¯å¾„
            path = Path(file_path).resolve()
            
            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åœ¨å…è®¸çš„è·¯å¾„å†…
            if not self._is_path_allowed(path):
                allowed_paths_str = '\n'.join([f"  - {p}" for p in self.allowed_paths])
                error_msg = self._('file_read_access_denied', default="Access denied: {path} is outside allowed directories.\n\nAllowed directories:\n{dirs}\n\nPlease check the file path format and try again with a path within the allowed directories.", path=path, dirs=allowed_paths_str)
                
                return ToolResult(
                    error=error_msg,
                    llm_content=error_msg
                )
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not path.exists():
                return ToolResult(
                    error=self._('file_read_not_found', default="File not found: {path}", path=path)
                )
                
            if not path.is_file():
                return ToolResult(
                    error=self._('file_read_not_file', default="Path is not a file: {path}", path=path)
                )
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = path.stat().st_size
            max_size = self.MAX_TEXT_FILE_SIZE if not self._is_image(path) else self.MAX_IMAGE_FILE_SIZE
            if file_size > max_size:
                return ToolResult(
                    error=self._('file_read_too_large', default="File too large: {size} bytes (max: {max} bytes)", size=file_size, max=max_size)
                )
            
            # æ™ºèƒ½æ–‡ä»¶ç±»å‹æ£€æµ‹ï¼ˆå€Ÿé‰´Gemini CLIï¼‰
            if self._is_image(path):
                return await self._read_image(path)
            elif self._is_binary(path):
                return self._handle_binary_file(path)
            
            # åˆ†ææ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
            analysis = None
            if analyze:
                analysis = await self._analyze_file(path)
            
            # è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆæ”¯æŒåˆ†é¡µï¼‰
            try:
                content, lines_read, has_more = await self._read_file_content(
                    path, encoding, offset, limit
                )
            except Exception as read_error:
                # æ›´å‹å¥½çš„é”™è¯¯å¤„ç†
                return ToolResult(
                    error=self._('file_read_failed', default="Failed to read file: {error}", error=str(read_error)),
                    llm_content=self._('file_read_failed_llm', default="Error reading {path}: {error}", path=path, error=str(read_error)),
                    return_display=self._('file_read_failed_display', default="âŒ Failed to read file: {error}", error=str(read_error))
                )
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œç‰¹æ®Šå¤„ç†
            if path.suffix.lower() == '.sql':
                return self._handle_sql_file(content, path, lines_read, has_more, analysis)
            elif path.suffix.lower() in ['.json']:
                return self._handle_json_file(content, path, lines_read, has_more, analysis)
            elif path.suffix.lower() in ['.yaml', '.yml']:
                return self._handle_yaml_file(content, path, lines_read, has_more, analysis)
            elif path.suffix.lower() in ['.csv', '.tsv']:
                return self._handle_csv_file(content, path, lines_read, has_more, analysis)
            else:
                # é€šç”¨æ–‡æœ¬æ–‡ä»¶å¤„ç†
                return self._handle_text_file(content, path, lines_read, has_more, analysis, offset)
                
        except Exception as e:
            return ToolResult(
                error=f"Failed to read file: {str(e)}"
            )
    
    def _is_path_allowed(self, path: Path) -> bool:
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦åœ¨å…è®¸çš„ç›®å½•å†…"""
        for allowed_path in self.allowed_paths:
            allowed = Path(allowed_path).resolve()
            try:
                path.relative_to(allowed)
                return True
            except ValueError:
                continue
        return False
    
    async def _detect_encoding(self, file_path: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            # ä½¿ç”¨æ–°çš„ç¼–ç æ£€æµ‹å·¥å…·
            from ..utils.encoding_utils import get_file_encoding_candidates
            candidates = get_file_encoding_candidates()
            
            # è¯»å–æ–‡ä»¶å‰å‡ KBè¿›è¡Œæ£€æµ‹
            path = Path(file_path)
            if path.exists() and path.is_file():
                with open(path, 'rb') as f:
                    sample = f.read(10240)  # è¯»å–å‰10KB
                    
                # å°è¯•ä½¿ç”¨chardetï¼ˆå¦‚æœå¯ç”¨ï¼‰
                try:
                    import chardet
                    result = chardet.detect(sample)
                    if result and result['encoding'] and result['confidence'] > 0.7:
                        detected = result['encoding'].lower()
                        # ä½¿ç”¨ encoding_utils çš„æ ‡å‡†åŒ–åŠŸèƒ½
                        try:
                            from ..utils.encoding_utils import EncodingDetector
                            return EncodingDetector.normalize_encoding(detected)
                        except:
                            # åå¤‡æ–¹æ¡ˆï¼šç®€å•çš„æ ‡å‡†åŒ–
                            if detected in ['shift_jis', 'shift-jis']:
                                return 'shift_jis'
                            elif detected in ['euc-jp', 'euc_jp']:
                                return 'euc_jp'
                            elif detected in ['gb2312', 'gb18030']:
                                return 'gbk'
                            return detected
                except ImportError:
                    pass
                    
                # å¦‚æœchardetä¸å¯ç”¨æˆ–ä¸ç¡®å®šï¼Œå°è¯•å€™é€‰ç¼–ç 
                for encoding in candidates[:5]:  # åªå°è¯•å‰5ä¸ªæœ€å¯èƒ½çš„
                    try:
                        sample.decode(encoding)
                        return encoding
                    except:
                        continue
                        
        except Exception as e:
            if DebugLogger.should_log("DEBUG"):
                log_info("FileReadTool", f"ç¼–ç æ£€æµ‹å¤±è´¥: {str(e)}")
                
        # é»˜è®¤ä½¿ç”¨ç³»ç»Ÿç¼–ç 
        try:
            from ..utils.encoding_utils import get_system_encoding
            return get_system_encoding()
        except:
            return "utf-8"
    
    def _is_image(self, path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡ä»¶"""
        return path.suffix.lower() in self.IMAGE_EXTENSIONS
    
    def _is_binary(self, path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆå€Ÿé‰´Gemini CLIï¼‰"""
        if path.suffix.lower() in self.TEXT_EXTENSIONS:
            return False
        
        # é€šè¿‡MIMEç±»å‹åˆ¤æ–­
        mime_type, _ = mimetypes.guess_type(str(path))
        if mime_type and mime_type.startswith('text/'):
            return False
        
        # è¯»å–å‰4KBæ£€æŸ¥å†…å®¹
        try:
            with open(path, 'rb') as f:
                chunk = f.read(4096)
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç©ºå­—èŠ‚
                if b'\x00' in chunk:
                    return True
                # å°è¯•è§£ç ä¸ºUTF-8
                try:
                    chunk.decode('utf-8')
                    return False
                except UnicodeDecodeError:
                    return True
        except:
            return True
    
    async def _read_image(self, path: Path) -> ToolResult:
        """è¯»å–å›¾ç‰‡æ–‡ä»¶ï¼ˆå€Ÿé‰´Gemini CLIï¼‰"""
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if path.stat().st_size > self.MAX_IMAGE_FILE_SIZE:
                return ToolResult(
                    error=f"Image file too large: {self._format_size(path.stat().st_size)} (max: {self._format_size(self.MAX_IMAGE_FILE_SIZE)})"
                )
            
            # è¯»å–å¹¶è½¬æ¢ä¸ºbase64
            with open(path, 'rb') as f:
                image_data = f.read()
            
            base64_data = base64.b64encode(image_data).decode('utf-8')
            mime_type, _ = mimetypes.guess_type(str(path))
            
            return ToolResult(
                summary=self._('file_read_image_summary', default="Read image file: {filename}", filename=path.name),
                llm_content=self._('file_read_image_llm', default="[Image file: {filename}, type: {type}, size: {size}]", filename=path.name, type=mime_type, size=self._format_size(len(image_data))),
                return_display=self._('file_read_image_display', default="ğŸ–¼ï¸ {filename}\nğŸ“Š Type: {type}\nğŸ’¾ Size: {size}", filename=path.name, type=mime_type, size=self._format_size(len(image_data)))
            )
        except Exception as e:
            return ToolResult(
                error=self._('file_read_image_failed', default="Failed to read image: {error}", error=str(e))
            )
    
    def _handle_binary_file(self, path: Path) -> ToolResult:
        """å¤„ç†äºŒè¿›åˆ¶æ–‡ä»¶"""
        file_size = path.stat().st_size
        mime_type, _ = mimetypes.guess_type(str(path))
        
        return ToolResult(
            summary=self._('file_read_binary_summary', default="Binary file: {filename}", filename=path.name),
            llm_content=self._('file_read_binary_llm', default="[Binary file: {filename}, type: {type}, size: {size} bytes]", filename=path.name, type=mime_type or 'unknown', size=file_size),
            return_display=self._('file_read_binary_display', default="ğŸ”’ Binary file\nğŸ“„ {filename}\nğŸ“Š Type: {type}\nğŸ’¾ Size: {size}", filename=path.name, type=mime_type or self._('file_read_unknown_type', default='unknown'), size=self._format_size(file_size))
        )
    
    async def _read_file_content(self, path: Path, encoding: str, offset: int, limit: int) -> tuple[str, int, bool]:
        """å¼‚æ­¥è¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒåˆ†é¡µï¼ˆå€Ÿé‰´Gemini CLIï¼‰"""
        import aiofiles
        
        lines_output = []
        lines_read = 0
        has_more = False
        total_lines = 0
        
        async with aiofiles.open(path, mode='r', encoding=encoding) as f:
            # é¦–å…ˆè¯»å–æ‰€æœ‰è¡Œä»¥è·å–æ€»è¡Œæ•°ï¼ˆå‚è€ƒGemini CLIçš„åšæ³•ï¼‰
            # å¯¹äºå¤§æ–‡ä»¶ï¼Œè¿™å¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ï¼Œä½†èƒ½ä¿è¯æ­£ç¡®æ€§
            all_lines = await f.readlines()
            total_lines = len(all_lines)
            
            # å‚è€ƒGemini CLIï¼šä¿æŠ¤offsetä¸è¶…è¿‡æ–‡ä»¶æ€»è¡Œæ•°
            actual_offset = min(offset, total_lines)
            
            # è®¡ç®—å®é™…çš„ç»“æŸè¡Œ
            end_line = min(actual_offset + limit, total_lines)
            
            # è·å–éœ€è¦çš„è¡Œ
            selected_lines = all_lines[actual_offset:end_line]
            
            # å¤„ç†æ¯ä¸€è¡Œ
            for i, line in enumerate(selected_lines):
                # è¡Œé•¿åº¦é™åˆ¶ï¼ˆå€Ÿé‰´Gemini CLIï¼‰
                if len(line) > self.MAX_LINE_LENGTH:
                    line = line[:self.MAX_LINE_LENGTH] + self._('file_read_line_truncated', default='... [truncated]\n')
                
                # æ·»åŠ è¡Œå·ï¼ˆcat -n é£æ ¼ï¼Œä½†ä½¿ç”¨å®é™…çš„è¡Œå·ï¼‰
                line_number = actual_offset + i + 1
                # æ ¼å¼åŒ–è¡Œå·ï¼Œä¿è¯å¯¹é½ï¼ˆæœ€å¤š6ä½æ•°ï¼‰
                lines_output.append(f"{line_number:6d}\t{line}")
                lines_read += 1
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šå†…å®¹
            has_more = end_line < total_lines
        
        # å¦‚æœæ²¡æœ‰è¯»å–åˆ°ä»»ä½•å†…å®¹ï¼ˆå¯èƒ½æ˜¯offsetè¶…å‡ºèŒƒå›´ï¼‰ï¼Œè¿”å›å‹å¥½æç¤º
        if not lines_output and offset >= total_lines:
            return self._('file_read_offset_out_of_range', default="[File only has {total} lines, but requested to start from line {line}]\n", total=total_lines, line=offset + 1), 0, False
        
        return ''.join(lines_output), lines_read, has_more
    
    def _handle_sql_file(self, content: str, path: Path, lines_read: int, has_more: bool, analysis: Optional[FileAnalysisResult]) -> ToolResult:
        """å¤„ç†SQLæ–‡ä»¶"""
        # åˆ†æSQLå†…å®¹
        statement_count = content.count(';')
        
        # æ™ºèƒ½æ£€æµ‹SQLç±»å‹ - ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è€Œéç®€å•åŒ…å«
        sql_types = []
        content_upper = content.upper()
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æµ‹SQLè¯­å¥ç±»å‹ï¼Œé¿å…è¯¯åˆ¤
        import re
        sql_patterns = {
            'SELECT': r'\bSELECT\s+',
            'INSERT': r'\bINSERT\s+INTO\s+',
            'UPDATE': r'\bUPDATE\s+',
            'DELETE': r'\bDELETE\s+FROM\s+',
            'CREATE': r'\bCREATE\s+(TABLE|INDEX|VIEW|DATABASE)\s+',
            'ALTER': r'\bALTER\s+(TABLE|INDEX|VIEW)\s+',
            'DROP': r'\bDROP\s+(TABLE|INDEX|VIEW|DATABASE)\s+'
        }
        
        for sql_type, pattern in sql_patterns.items():
            if re.search(pattern, content_upper):
                sql_types.append(sql_type)
        
        summary = self._('file_read_sql_summary', default="Read SQL script: {filename} ({lines} lines)", filename=path.name, lines=lines_read)
        if has_more:
            summary += self._('file_read_partial_suffix', default=" [partial content]")
        
        llm_content = self._('file_read_sql_content', default="SQLè„šæœ¬å†…å®¹:\n\n{content}", content=content)
        if has_more:
            llm_content += self._('file_read_more_content', default="\n\n[æ–‡ä»¶è¿˜æœ‰æ›´å¤šå†…å®¹ï¼Œä½¿ç”¨offsetå’Œlimitå‚æ•°åˆ†é¡µè¯»å–]")
        
        display_lines = [
            f"ğŸ“„ {path.name}",
            self._('file_read_sql_statements', default="ğŸ“Š Statements: ~{count}", count=statement_count),
            self._('file_read_sql_types', default="ğŸ“ Types: {types}", types=', '.join(sql_types) if sql_types else self._('file_read_unknown', default='unknown')),
            self._('file_read_lines_read', default="ğŸ“ Lines read: {lines}", lines=lines_read)
        ]
        
        if has_more:
            display_lines.append(self._('file_read_has_more', default="âš ï¸ File has more content"))
        
        if analysis:
            display_lines.append(self._('file_read_file_size', default="ğŸ’¾ File size: {size}", size=self._format_size(analysis.file_size)))
        
        return ToolResult(
            summary=summary,
            llm_content=llm_content,
            return_display="\n".join(display_lines)
        )
    
    def _handle_json_file(self, content: str, path: Path, lines_read: int, has_more: bool, analysis: Optional[FileAnalysisResult]) -> ToolResult:
        """å¤„ç†JSONæ–‡ä»¶"""
        try:
            # å¦‚æœå†…å®¹è¢«æˆªæ–­ï¼Œä¸å°è¯•è§£æ
            if has_more:
                return ToolResult(
                    summary=self._('file_read_json_partial', default="Read JSON file: {filename} ({lines} lines) [partial content]", filename=path.name, lines=lines_read),
                    llm_content=self._('file_read_json_partial_llm', default="JSON file partial content:\n\n{content}\n\n[File truncated, complete parsing requires reading all content]", content=content),
                    return_display=self._('file_read_json_partial_display', default="ğŸ“„ {filename}\nğŸ“ Lines read: {lines}\nâš ï¸ Content truncated, cannot parse structure", filename=path.name, lines=lines_read)
                )
            
            # æ¸…ç†è¡Œå·å‰ç¼€ï¼ˆæ›´çµæ´»çš„å¤„ç†ï¼‰
            clean_content = '\n'.join(
                line.split('\t', 1)[1] if '\t' in line else line 
                for line in content.split('\n') if line.strip()
            )
            data = json.loads(clean_content)
            
            # ç”Ÿæˆç»“æ„æ‘˜è¦
            def get_structure(obj, level=0):
                if level > 2:  # é™åˆ¶æ·±åº¦
                    return "..."
                if isinstance(obj, dict):
                    return {k: get_structure(v, level+1) for k, v in list(obj.items())[:5]}
                elif isinstance(obj, list) and obj:
                    return f"Array[{len(obj)}]" if len(obj) > 1 else [get_structure(obj[0], level+1)]
                else:
                    return type(obj).__name__
            
            structure = get_structure(data)
            
            return ToolResult(
                summary=self._('file_read_json_summary', default="Read JSON file: {filename}", filename=path.name),
                llm_content=self._('file_read_json_llm', default="JSON content:\n\n{content}", content=json.dumps(data, indent=2, ensure_ascii=False)),
                return_display=self._('file_read_json_display', default="ğŸ“„ {filename}\nğŸ“Š Structure: {structure}...\nğŸ“ Lines: {lines}", filename=path.name, structure=json.dumps(structure, indent=2, ensure_ascii=False)[:200], lines=lines_read)
            )
        except json.JSONDecodeError as e:
            return ToolResult(
                summary=self._('file_read_json_invalid', default="Invalid JSON file"),
                llm_content=self._('file_read_json_error_llm', default="JSON parse error {filename}: {error}\n\nContent:\n{content}", filename=path.name, error=str(e), content=content),
                return_display=self._('file_read_json_error_display', default="âŒ JSON parse error: {error}", error=str(e))
            )
    
    def _handle_yaml_file(self, content: str, path: Path, lines_read: int, has_more: bool, analysis: Optional[FileAnalysisResult]) -> ToolResult:
        """å¤„ç†YAMLæ–‡ä»¶"""
        try:
            # å¦‚æœå†…å®¹è¢«æˆªæ–­ï¼Œä¸å°è¯•è§£æ
            if has_more:
                return ToolResult(
                    summary=self._('file_read_yaml_partial', default="Read YAML file: {filename} ({lines} lines) [partial content]", filename=path.name, lines=lines_read),
                    llm_content=self._('file_read_yaml_partial_llm', default="YAML file partial content:\n\n{content}\n\n[File truncated, complete parsing requires reading all content]", content=content),
                    return_display=self._('file_read_yaml_partial_display', default="ğŸ“„ {filename}\nğŸ“ Lines read: {lines}\nâš ï¸ Content truncated, cannot parse structure", filename=path.name, lines=lines_read)
                )
            
            # ç§»é™¤è¡Œå·åè§£æï¼ˆæ›´å¥å£®çš„å¤„ç†ï¼‰
            clean_content = '\n'.join(
                line.split('\t', 1)[1] if '\t' in line else line 
                for line in content.split('\n')
            )
            data = yaml.safe_load(clean_content)
            
            keys_info = self._('file_read_yaml_unknown_structure', default="Unknown structure")
            if isinstance(data, dict):
                keys_info = self._('file_read_yaml_top_keys', default="Top keys: {keys}", keys=', '.join(list(data.keys())[:10]))
                if len(data.keys()) > 10:
                    keys_info += self._('file_read_yaml_more_keys', default=" ... (total {count})", count=len(data.keys()))
            elif isinstance(data, list):
                keys_info = self._('file_read_yaml_array', default="Array with {count} elements", count=len(data))
            
            return ToolResult(
                summary=self._('file_read_yaml_summary', default="Read YAML config file: {filename}", filename=path.name),
                llm_content=self._('file_read_yaml_llm', default="YAML content:\n\n{content}", content=content),
                return_display=f"ğŸ“„ {path.name}\nğŸ“Š {keys_info}\nè¡Œæ•°: {lines_read}"
            )
        except yaml.YAMLError as e:
            return ToolResult(
                summary=self._('file_read_yaml_invalid', default="Invalid YAML file"),
                llm_content=self._('file_read_yaml_error_llm', default="YAML parse error {filename}: {error}\n\nContent:\n{content}", filename=path.name, error=str(e), content=content),
                return_display=self._('file_read_yaml_error_display', default="âŒ YAML parse error: {error}", error=str(e))
            )
    
    def _handle_csv_file(self, content: str, path: Path, lines_read: int, has_more: bool, analysis: Optional[FileAnalysisResult]) -> ToolResult:
        """å¤„ç†CSVæ–‡ä»¶"""
        lines = content.strip().split('\n')
        
        if lines:
            # æå–è¡¨å¤´ï¼ˆç¬¬ä¸€è¡Œï¼Œå»é™¤è¡Œå·ï¼‰
            header_line = lines[0].split('\t', 1)[1] if '\t' in lines[0] else lines[0]
            # æ™ºèƒ½æ£€æµ‹åˆ†éš”ç¬¦
            delimiter = '\t' if '\t' in header_line else ','
            headers = [h.strip() for h in header_line.split(delimiter)]
            
            row_count = lines_read - 1  # å‡å»è¡¨å¤´
            
            # åˆ†ææ•°æ®æ ·æœ¬
            sample_rows = []
            for line in lines[1:6]:  # æœ€å¤š5è¡Œæ ·æœ¬
                if '\t' in line:
                    line = line.split('\t', 1)[1]
                sample_rows.append(line.split(delimiter))
            
            summary = self._('file_read_csv_summary', default="Read CSV file: {filename} ({rows} rows data)", filename=path.name, rows=row_count)
            if has_more:
                summary += self._('file_read_partial_suffix', default=" [partial content]")
            
            llm_content = self._('file_read_csv_llm', default="CSV file content:\n\n{content}", content=content)
            if has_more:
                llm_content += self._('file_read_more_data_hint', default="\n\n[File has more data, use offset and limit parameters for pagination]")
            
            display_lines = [
                f"ğŸ“„ {path.name}",
                self._('file_read_csv_columns', default="ğŸ“Š Columns: {count}", count=len(headers)),
                self._('file_read_csv_headers', default="ğŸ“‹ Headers: {headers}{more}", headers=', '.join(headers[:5]), more='...' if len(headers) > 5 else ''),
                self._('file_read_csv_rows', default="ğŸ“ Data rows: {count}", count=row_count)
            ]
            
            if has_more:
                display_lines.append(self._('file_read_more_data', default="âš ï¸ File has more data"))
            
            if analysis:
                display_lines.append(self._('file_read_file_size', default="ğŸ’¾ File size: {size}", size=self._format_size(analysis.file_size)))
            
            return ToolResult(
                summary=summary,
                llm_content=llm_content,
                return_display="\n".join(display_lines)
            )
        else:
            return ToolResult(
                summary=self._('file_read_csv_empty', default="Empty CSV file"),
                llm_content=self._('file_read_csv_empty_llm', default="Empty CSV file: {filename}", filename=path.name),
                return_display=self._('file_read_csv_empty_display', default="ğŸ“„ Empty CSV file")
            )
    
    def _handle_text_file(self, content: str, path: Path, lines_read: int, has_more: bool, analysis: Optional[FileAnalysisResult], offset: int = 0) -> ToolResult:
        """å¤„ç†é€šç”¨æ–‡æœ¬æ–‡ä»¶"""
        # æ›´çµæ´»çš„æ‘˜è¦ç”Ÿæˆ
        summary_parts = [self._('file_read_text_read', default="Read {filename}", filename=path.name)]
        if offset > 0:
            summary_parts.append(self._('file_read_text_from_line', default="from line {line}", line=offset + 1))
        summary_parts.append(self._('file_read_text_lines', default="{lines} lines", lines=lines_read))
        if has_more:
            summary_parts.append(self._('file_read_text_partial', default="partial content"))
        summary = ", ".join(summary_parts)
        
        # æ”¹è¿›çš„LLMå†…å®¹æ ¼å¼
        llm_content = content
        if offset > 0 or has_more:
            # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = []
            if offset > 0:
                context_info.append(self._('file_read_from_line_context', default="from line {line}", line=offset + 1))
            if has_more:
                context_info.append(self._('file_read_has_more_context', default="file has more content"))
            llm_content = self._('file_read_partial_content', default="[File partial content: {context}]\n\n{content}", context=', '.join(context_info), content=content)
            if has_more:
                llm_content += self._('file_read_use_pagination', default="\n[Use offset and limit parameters to read more content]")
        
        # æ”¹è¿›çš„æ˜¾ç¤ºå†…å®¹
        display_lines = [
            f"ğŸ“„ {path.name}"
        ]
        
        if offset > 0:
            display_lines.append(self._('file_read_start_from', default="ğŸ“– Start from line {line}", line=offset + 1))
        
        display_lines.append(f"è¡Œæ•°: {lines_read}")
        
        if has_more:
            display_lines.append(self._('file_read_has_more', default="âš ï¸ File has more content"))
        
        if analysis:
            display_lines.append(self._('file_read_file_size', default="ğŸ’¾ File size: {size}", size=self._format_size(analysis.file_size)))
            if analysis.detected_encoding:
                display_lines.append(self._('file_read_encoding', default="ğŸ”¤ Encoding: {encoding}", encoding=analysis.detected_encoding))
        
        return ToolResult(
            summary=summary,
            llm_content=llm_content,
            return_display="\n".join(display_lines)
        )
    
    async def _analyze_file(self, path: Path) -> FileAnalysisResult:
        """åˆ†ææ–‡ä»¶ç»“æ„å’Œå†…å®¹"""
        import aiofiles
        
        # chardetæ˜¯å¯é€‰ä¾èµ–ï¼Œä¼˜é›…é™çº§
        try:
            import chardet
        except ImportError:
            chardet = None
        
        result = FileAnalysisResult(
            file_path=str(path),
            file_size=path.stat().st_size
        )
        
        # æ£€æµ‹ç¼–ç  - åªæœ‰chardetå¯ç”¨æ—¶æ‰æ£€æµ‹
        if chardet:
            try:
                with open(path, 'rb') as f:
                    raw_data = f.read(10000)  # è¯»å–å‰10KBç”¨äºç¼–ç æ£€æµ‹
                    detected = chardet.detect(raw_data)
                    result.detected_encoding = detected.get('encoding', 'unknown')
            except:
                pass
        else:
            result.detected_encoding = 'utf-8'  # é»˜è®¤å‡è®¾UTF-8
        
        # æ£€æµ‹æ ¼å¼
        ext = path.suffix.lower()
        if ext == '.csv':
            result.detected_format = FileFormat.CSV
            # TODO: åˆ†æCSVç»“æ„
        elif ext == '.json':
            result.detected_format = FileFormat.JSON
        elif ext in ['.yaml', '.yml']:
            result.detected_format = FileFormat.YAML
        elif ext == '.sql':
            result.detected_format = FileFormat.SQL
        
        return result
    
    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} TB"