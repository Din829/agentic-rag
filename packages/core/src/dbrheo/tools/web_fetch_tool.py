"""
ç½‘é¡µå†…å®¹è·å–å·¥å…· - å‚è€ƒ Gemini CLI çš„ web-fetch å®ç°
å¯ä»¥è·å– URL çš„å®é™…å†…å®¹ï¼Œå¹¶è½¬æ¢ä¸ºæ–‡æœ¬
"""

import aiohttp
import re
from typing import Dict, Any, Optional, List
from urllib.parse import urlparse, quote
from bs4 import BeautifulSoup
from ..types.tool_types import ToolResult
from ..types.core_types import AbortSignal
from .base import Tool
from ..config.base import AgentConfig
from ..utils.retry_with_backoff import retry_with_backoff, RetryOptions


class WebFetchTool(Tool):
    """
    ç½‘é¡µå†…å®¹è·å–å·¥å…·
    - ä» URL è·å–å®é™…å†…å®¹
    - æ”¯æŒ HTML è½¬æ–‡æœ¬
    - æ”¯æŒå¤šä¸ª URLï¼ˆæœ€å¤š 20 ä¸ªï¼‰
    - çµæ´»å¤„ç†å„ç§ç½‘é¡µæ ¼å¼
    """
    
    # é»˜è®¤è®¾ç½®
    URL_FETCH_TIMEOUT = 10  # ç§’
    MAX_CONTENT_LENGTH = 100000  # 100KB
    MAX_URLS = 20
    
    def __init__(self, config: AgentConfig, i18n=None):
        # å…ˆä¿å­˜i18nå®ä¾‹ï¼Œä»¥ä¾¿åœ¨åˆå§‹åŒ–æ—¶ä½¿ç”¨
        self._i18n = i18n
        super().__init__(
            name="web_fetch",
            display_name=self._('web_fetch_tool_name', default="ç½‘é¡µå†…å®¹è·å–") if i18n else "ç½‘é¡µå†…å®¹è·å–",
            description="Fetch and extract content from web pages. Use this after web_search to read the actual content of search results. Provide URLs and optional instructions for processing (e.g., 'summarize the key points').",
            parameter_schema={
                "type": "object",
                "properties": {
                    "prompt": {
                        "type": "string",
                        "description": "The prompt containing URL(s) and instructions for processing the content"
                    },
                    "urls": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Direct list of URLs to fetch (alternative to embedding in prompt)"
                    },
                    "extract_text": {
                        "type": "boolean",
                        "description": "Convert HTML to plain text (default: true)"
                    },
                    "selector": {
                        "type": "string",
                        "description": "CSS selector to extract specific content"
                    }
                },
                "required": ["prompt"]
            },
            is_output_markdown=True,
            can_update_output=True,
            should_summarize_display=True,
            i18n=i18n  # ä¼ é€’i18nç»™åŸºç±»
        )
        self.config = config
    
    def validate_tool_params(self, params: Dict[str, Any]) -> Optional[str]:
        """éªŒè¯å‚æ•°"""
        # æ ‡å‡†åŒ–å‚æ•°ï¼Œå¤„ç† protobuf å¯¹è±¡
        params = self._normalize_params(params)
        
        prompt = params.get("prompt", "")
        urls = params.get("urls", [])
        
        # ä» prompt ä¸­æå– URLs
        extracted_urls = self._extract_urls(prompt)
        
        # åˆå¹¶æ‰€æœ‰ URLs
        all_urls = list(set(urls + extracted_urls))
        
        if not all_urls:
            return self._('web_fetch_no_urls', default="No URLs found in prompt or urls parameter")
            
        if len(all_urls) > self.MAX_URLS:
            return self._('web_fetch_too_many_urls', default="Too many URLs (max {max})", max=self.MAX_URLS)
            
        # éªŒè¯ URL æ ¼å¼
        for url in all_urls:
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return self._('web_fetch_invalid_url', default="Invalid URL: {url}", url=url)
                
        return None
    
    def get_description(self, params: Dict[str, Any]) -> str:
        """è·å–æ“ä½œæè¿°"""
        # æ ‡å‡†åŒ–å‚æ•°
        params = self._normalize_params(params)
        
        prompt = params.get("prompt", "")
        urls = params.get("urls", [])
        extracted_urls = self._extract_urls(prompt)
        all_urls = list(set(urls + extracted_urls))
        
        if len(all_urls) == 1:
            return self._('web_fetch_desc_single', default="è·å–ç½‘é¡µå†…å®¹: {url}...", url=all_urls[0][:50])
        else:
            return self._('web_fetch_desc_multiple', default="è·å– {count} ä¸ªç½‘é¡µçš„å†…å®¹", count=len(all_urls))
    
    async def should_confirm_execute(self, params: Dict[str, Any], signal: AbortSignal) -> Optional[Any]:
        """ç½‘é¡µè·å–é€šå¸¸ä¸éœ€è¦ç¡®è®¤ï¼Œé™¤éæ˜¯å†…ç½‘åœ°å€"""
        # æ ‡å‡†åŒ–å‚æ•°
        params = self._normalize_params(params)
        
        prompt = params.get("prompt", "")
        urls = params.get("urls", [])
        extracted_urls = self._extract_urls(prompt)
        all_urls = list(set(urls + extracted_urls))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å†…ç½‘åœ°å€
        for url in all_urls:
            if self._is_private_url(url):
                return {
                    "title": self._('web_fetch_confirm_private', default="è®¿é—®å†…ç½‘åœ°å€éœ€è¦ç¡®è®¤"),
                    "url": url,
                    "risk": self._('web_fetch_risk_private', default="è®¿é—®å†…éƒ¨ç½‘ç»œèµ„æº")
                }
        
        return False
    
    async def execute(
        self,
        params: Dict[str, Any],
        signal: AbortSignal,
        update_output: Optional[Any] = None
    ) -> ToolResult:
        """æ‰§è¡Œç½‘é¡µå†…å®¹è·å–"""
        # æ ‡å‡†åŒ–å‚æ•°ï¼Œç¡®ä¿ protobuf å¯¹è±¡è¢«æ­£ç¡®è½¬æ¢
        params = self._normalize_params(params)
        
        prompt = params.get("prompt", "")
        urls = params.get("urls", [])
        extract_text = params.get("extract_text", True)
        selector = params.get("selector", None)
        
        # æå–æ‰€æœ‰ URLs
        extracted_urls = self._extract_urls(prompt)
        all_urls = list(set(urls + extracted_urls))[:self.MAX_URLS]
        
        if not all_urls:
            return ToolResult(
                error=self._('web_fetch_no_urls_error', default="No URLs found to fetch")
            )
        
        results = []
        errors = []
        
        # è·å–æ¯ä¸ª URL çš„å†…å®¹
        for i, url in enumerate(all_urls):
            if update_output:
                update_output(self._('web_fetch_progress', default="ğŸŒ è·å–ç½‘é¡µ {current}/{total}: {url}", current=i+1, total=len(all_urls), url=url))
            
            try:
                content = await self._fetch_url(url, extract_text, selector)
                results.append({
                    "url": url,
                    "content": content,
                    "success": True
                })
            except Exception as e:
                # å¢å¼ºé”™è¯¯ä¿¡æ¯è¯Šæ–­ï¼Œæä¾›å¼‚å¸¸ç±»å‹å’Œä¸Šä¸‹æ–‡
                error_msg = self._format_error_message(e, url)
                results.append({
                    "url": url,
                    "error": error_msg,
                    "success": False
                })
                errors.append(f"{url}: {error_msg}")
        
        # æ ¼å¼åŒ–ç»“æœ
        if not results:
            return ToolResult(
                error=self._('web_fetch_all_failed', default="Failed to fetch any content")
            )
        
        # æ„å»ºå“åº”
        llm_content = self._format_results_for_llm(results, prompt)
        display_content = self._format_results_for_display(results)
        
        success_count = len([r for r in results if r['success']])
        summary = self._('web_fetch_summary', default="è·å–äº† {count} ä¸ªç½‘é¡µçš„å†…å®¹", count=success_count)
        if errors:
            summary += self._('web_fetch_summary_errors', default="ï¼Œ{count} ä¸ªå¤±è´¥", count=len(errors))
        
        return ToolResult(
            summary=summary,
            llm_content=llm_content,
            return_display=display_content
        )
    
    def _extract_urls(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå– URLs"""
        # æ”¹è¿›çš„ URL æ­£åˆ™è¡¨è¾¾å¼
        url_pattern = r'https?://(?:www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b(?:[-a-zA-Z0-9()@:%_\+.~#?&/=]*)'
        urls = re.findall(url_pattern, text)
        return list(set(urls))  # å»é‡
    
    def _is_private_url(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå†…ç½‘åœ°å€"""
        parsed = urlparse(url)
        host = parsed.hostname
        
        if not host:
            return False
            
        # æœ¬åœ°åœ°å€
        if host in ['localhost', '127.0.0.1', '::1']:
            return True
            
        # ç§æœ‰ IP èŒƒå›´
        private_patterns = [
            r'^10\.',
            r'^172\.(1[6-9]|2[0-9]|3[0-1])\.',
            r'^192\.168\.'
        ]
        
        for pattern in private_patterns:
            if re.match(pattern, host):
                return True
                
        return False
    
    async def _fetch_url(self, url: str, extract_text: bool = True, selector: Optional[str] = None) -> str:
        """è·å–å•ä¸ª URL çš„å†…å®¹ï¼Œæ”¯æŒè½»é‡é‡è¯•æœºåˆ¶"""
        
        # Webè¯·æ±‚ä¸“ç”¨çš„è½»é‡é‡è¯•é…ç½®
        retry_options = RetryOptions(
            max_attempts=2,        # æ€»å…±2æ¬¡å°è¯•ï¼ˆ1æ¬¡é‡è¯•ï¼‰
            initial_delay_ms=1000, # 1ç§’åˆå§‹å»¶è¿Ÿ
            max_delay_ms=3000      # æœ€å¤§3ç§’å»¶è¿Ÿ
        )
        
        async def fetch_with_session():
            async with aiohttp.ClientSession() as session:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
                }
                
                timeout = aiohttp.ClientTimeout(total=self.URL_FETCH_TIMEOUT)
                
                async with session.get(url, headers=headers, timeout=timeout) as response:
                    if response.status != 200:
                        raise Exception(f"HTTP {response.status}")
                    
                    # æ£€æŸ¥å†…å®¹é•¿åº¦
                    content_length = response.headers.get('Content-Length')
                    if content_length and int(content_length) > self.MAX_CONTENT_LENGTH:
                        raise Exception(f"Content too large: {content_length} bytes")
                    
                    # è¯»å–å†…å®¹
                    content = await response.text()
                    
                    # é™åˆ¶å†…å®¹é•¿åº¦
                    if len(content) > self.MAX_CONTENT_LENGTH:
                        content = content[:self.MAX_CONTENT_LENGTH]
                    
                    # å¤„ç†å†…å®¹
                    if extract_text and response.content_type and 'html' in response.content_type:
                        content = self._html_to_text(content, selector)
                    
                    return content
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰§è¡Œè¯·æ±‚
        return await retry_with_backoff(fetch_with_session, retry_options)
    
    def _html_to_text(self, html: str, selector: Optional[str] = None) -> str:
        """å°† HTML è½¬æ¢ä¸ºçº¯æ–‡æœ¬"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # ç§»é™¤ script å’Œ style æ ‡ç­¾
        for script in soup(["script", "style"]):
            script.decompose()
        
        # å¦‚æœæŒ‡å®šäº†é€‰æ‹©å™¨ï¼Œåªæå–ç‰¹å®šå†…å®¹
        if selector:
            selected = soup.select(selector)
            if selected:
                text_parts = []
                for elem in selected:
                    text = elem.get_text(strip=True, separator=' ')
                    if text:
                        text_parts.append(text)
                return '\n\n'.join(text_parts)
        
        # æå–æ‰€æœ‰æ–‡æœ¬
        text = soup.get_text(strip=True, separator=' ')
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½
        lines = [line.strip() for line in text.split('\n')]
        lines = [line for line in lines if line]
        
        return '\n'.join(lines)
    
    def _format_error_message(self, exception: Exception, url: str) -> str:
        """
        æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯ï¼Œæä¾›æ›´è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
        é€šç”¨çš„é”™è¯¯å¢å¼ºæ–¹æ³•ï¼Œé€‚ç”¨äºå„ç§å¼‚å¸¸ç±»å‹
        """
        import aiohttp
        import asyncio
        from urllib.parse import urlparse
        
        exception_type = type(exception).__name__
        error_detail = str(exception).strip()
        
        # æ„å»ºåŸºç¡€é”™è¯¯ä¿¡æ¯
        if error_detail:
            base_msg = f"{exception_type}: {error_detail}"
        else:
            base_msg = f"{exception_type}: (no specific details)"
        
        # æ ¹æ®å¼‚å¸¸ç±»å‹æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = []
        
        # HTTPç›¸å…³é”™è¯¯
        if isinstance(exception, aiohttp.ClientError):
            if isinstance(exception, aiohttp.ClientTimeout):
                context_info.append("connection/read timeout")
            elif isinstance(exception, aiohttp.ClientConnectorError):
                context_info.append("connection failed")
            elif isinstance(exception, aiohttp.ClientSSLError):
                context_info.append("SSL/TLS certificate issue")
            elif isinstance(exception, aiohttp.ClientResponseError):
                context_info.append(f"HTTP {getattr(exception, 'status', 'unknown')}")
        
        # ç½‘ç»œç›¸å…³é”™è¯¯
        elif isinstance(exception, (OSError, ConnectionError)):
            context_info.append("network connectivity issue")
        
        # è¶…æ—¶ç›¸å…³é”™è¯¯
        elif isinstance(exception, (asyncio.TimeoutError, TimeoutError)):
            context_info.append(f"timeout after {self.URL_FETCH_TIMEOUT}s")
        
        # è§£æURLè·å¾—é€šç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯
        try:
            parsed_url = urlparse(url)
            if parsed_url.scheme == 'https':
                context_info.append("HTTPS site")
        except:
            pass  # URLè§£æå¤±è´¥æ—¶å¿½ç•¥
        
        # ç»„åˆæœ€ç»ˆæ¶ˆæ¯
        if context_info:
            return f"{base_msg} ({', '.join(context_info)})"
        else:
            return base_msg
    
    def _format_results_for_llm(self, results: List[Dict], prompt: str) -> str:
        """æ ¼å¼åŒ–ç»“æœä¾› LLM ä½¿ç”¨"""
        content_parts = [f"Web fetch results for prompt: '{prompt}'\n"]
        
        for i, result in enumerate(results, 1):
            url = result['url']
            
            if result['success']:
                content = result['content']
                # é™åˆ¶æ¯ä¸ªå†…å®¹çš„é•¿åº¦
                if len(content) > 5000:
                    content = content[:5000] + self._('web_fetch_content_truncated', default="\n... [å†…å®¹å·²æˆªæ–­]")
                
                content_parts.append(f"\n=== Result {i}: {url} ===\n{content}")
            else:
                content_parts.append(f"\n=== Result {i}: {url} ===\nError: {result['error']}")
        
        # æ·»åŠ å¤„ç†æç¤º
        content_parts.append(f"\n\nBased on the fetched content, here's the analysis according to the prompt:")
        
        return '\n'.join(content_parts)
    
    def _format_results_for_display(self, results: List[Dict]) -> str:
        """æ ¼å¼åŒ–ç»“æœä¾›æ˜¾ç¤º"""
        lines = [self._('web_fetch_results_header', default="ğŸŒ ç½‘é¡µå†…å®¹è·å–ç»“æœ:\n")]
        
        success_count = len([r for r in results if r['success']])
        fail_count = len(results) - success_count
        
        if success_count > 0:
            lines.append(self._('web_fetch_success_count', default="âœ… æˆåŠŸè·å–: {count} ä¸ªç½‘é¡µ", count=success_count))
        if fail_count > 0:
            lines.append(self._('web_fetch_fail_count', default="âŒ è·å–å¤±è´¥: {count} ä¸ªç½‘é¡µ", count=fail_count))
        
        lines.append("")
        
        for i, result in enumerate(results, 1):
            url = result['url']
            if result['success']:
                content_preview = result['content'][:200] + "..." if len(result['content']) > 200 else result['content']
                lines.append(f"{i}. âœ… {url}")
                lines.append(self._('web_fetch_preview', default="   é¢„è§ˆ: {content}\n", content=content_preview))
            else:
                lines.append(f"{i}. âŒ {url}")
                lines.append(self._('web_fetch_error_line', default="   é”™è¯¯: {error}\n", error=result['error']))
        
        return '\n'.join(lines)